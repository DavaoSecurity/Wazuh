Use on a Rasp Pi5 8GB, so,

sudo apt install python3 python3-pip
pip3 install pandas scikit-learn wazuh-api joblib

Aim to create a tool that reads alerts from a Wazuh server and predicts threats using a machine learning model. 
Will use Python and popular libraries such as `pandas`, `scikit-learn`, and `wazuh-api` for interacting with the Wazuh server. 
Use `scikit-learn` for hyperparameter tuning and model selection. 

1. Set Up the Environment
First, ensure you have the necessary libraries installed.

pip install pandas scikit-learn wazuh-api


2. Connect to the Wazuh Server. Use the `wazuh-api` library to connect to the Wazuh server and fetch alerts.

<python>
from wazuh_api import WazuhAPI
import pandas as pd

# Replace with your Wazuh server details
WAZUH_URL = 'https://your-wazuh-server:55000'
WAZUH_USER = 'your-username'
WAZUH_PASSWORD = 'your-password'

# Initialize the Wazuh API client
wazuh = WazuhAPI(url=WAZUH_URL, user=WAZUH_USER, password=WAZUH_PASSWORD)

# Fetch alerts from the Wazuh server
alerts = wazuh.agents.alerts()

# Convert to DataFrame
df = pd.DataFrame(alerts)


3. Preprocess the Data
Filter alerts with a level greater than 8 and preprocess the data for training.

<python>
# Filter alerts with level greater than 8
df = df[df['level'] > 8]

# Select relevant features for training
features = ['agent_id', 'rule_id', 'rule_level', 'rule_message', 'data', 'full_log']
X = df[features]
y = df['threat']  # Assuming 'threat' is the target variable

# Encode categorical features
X = pd.get_dummies(X, drop_first=True)

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


4. Train the Model
Use Random Forest and tune hyperparameters using GridSearchCV.

<python>
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# Define the model
model = RandomForestClassifier(random_state=42)

# Define the hyperparameters to tune
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit the model
grid_search.fit(X_train, y_train)

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate the model
from sklearn.metrics import classification_report, accuracy_score

y_pred = best_model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))


5. Save the Model for future use.

<python>
import joblib

# Save the model to a file
joblib.dump(best_model, 'threatpredictmodel.pkl')


6. Load and Use the saved model and use it to predict threats from new alerts.

<python>
# Load the model
loaded_model = joblib.load('threatpredictmodel.pkl')

# Fetch new alerts from the Wazuh server
new_alerts = wazuh.agents.alerts()

# Preprocess new alerts
new_df = pd.DataFrame(new_alerts)
new_df = new_df[new_df['level'] > 8]
new_X = new_df[features]
new_X = pd.get_dummies(new_X, drop_first=True)

# Ensure the new data has the same columns as the training data
new_X = new_X.reindex(columns=X.columns, fill_value=0)

# Predict threats
new_predictions = loaded_model.predict(new_X)

# Add predictions to the DataFrame
new_df['predicted_threat'] = new_predictions

# Print the results
print(new_df[['agent_id', 'rule_id', 'predicted_threat']])


